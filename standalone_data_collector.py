"""
ç‹¬ç«‹å®æ—¶æ•°æ®é‡‡é›†å™¨
å¯ä»¥å•ç‹¬è¿è¡Œï¼ŒæŒç»­é‡‡é›†Kçº¿ã€é€ç¬”æˆäº¤ã€è®¢å•ç°¿æ•°æ®
ä¸äº¤æ˜“ç³»ç»Ÿè§£è€¦ï¼Œç¡®ä¿æ•°æ®åº“å§‹ç»ˆä¿æŒæœ€æ–°æ•°æ®
"""
import sys
import os
import signal
import asyncio
import argparse
import json
from tqdm import tqdm
import time
from datetime import datetime
from loguru import logger
from concurrent.futures import ThreadPoolExecutor
from collections import deque

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

from src.config.settings import config
from src.core.rest_client import OKXRestClient
from src.api.market import MarketAPI
from src.api.public import PublicAPI
from src.ai.data_manager import DataManager
from src.data.okx_websocket import OkxWebSocket


class StandaloneDataCollector:
    """ç‹¬ç«‹æ•°æ®é‡‡é›†å™¨"""

    # Kçº¿å‘¨æœŸåˆ°æ¯«ç§’çš„æ˜ å°„ï¼ˆç±»å¸¸é‡ï¼‰
    BAR_TO_MS = {
        '1m': 60 * 1000,
        '5m': 5 * 60 * 1000,
        '15m': 15 * 60 * 1000,
        '30m': 30 * 60 * 1000,
        '1H': 60 * 60 * 1000,
        '4H': 4 * 60 * 60 * 1000,
        '1D': 24 * 60 * 60 * 1000
    }

    # ä¸åŒå‘¨æœŸçš„å†å²æ•°æ®å¤©æ•°ï¼ˆç±»å¸¸é‡ï¼‰
    HISTORY_DAYS_BY_TIMEFRAME = {
        '1m': 3,      # 1åˆ†é’ŸKçº¿ï¼š3å¤©
        '5m': 7,      # 5åˆ†é’ŸKçº¿ï¼š7å¤©
        '15m': 15,    # 15åˆ†é’ŸKçº¿ï¼š15å¤©
        '30m': 30,    # 30åˆ†é’ŸKçº¿ï¼š30å¤©
        '1H': 30,     # 1å°æ—¶Kçº¿ï¼š30å¤©
        '4H': 30,     # 4å°æ—¶Kçº¿ï¼š30å¤©
        '1D': 30      # æ—¥çº¿ï¼š30å¤©
    }

    def __init__(
        self,
        symbols: list = None,
        timeframes: list = None,
        history_days: int = 30,
        data_timeout_seconds: int = 120
    ):
        """
        åˆå§‹åŒ–ç‹¬ç«‹æ•°æ®é‡‡é›†å™¨

        Args:
            symbols: äº¤æ˜“å¯¹åˆ—è¡¨ï¼ˆé»˜è®¤BTC-USDT-SWAPï¼‰
            timeframes: Kçº¿å‘¨æœŸåˆ—è¡¨ï¼ˆé»˜è®¤1m, 5m, 15mï¼‰
            history_days: åˆå§‹åŒ–å†å²æ•°æ®å¤©æ•°
            data_timeout_seconds: æ•°æ®æ›´æ–°è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œè¶…è¿‡æ­¤æ—¶é—´æœªæ”¶åˆ°æ•°æ®åˆ™é‡å¯ï¼ˆé»˜è®¤120ç§’ï¼‰
        """
        self.symbols = symbols or ['BTC-USDT-SWAP']
        self.timeframes = timeframes or ['1m', '5m', '15m']
        self.history_days = history_days
        self.data_timeout_seconds = data_timeout_seconds

        # è¿è¡ŒçŠ¶æ€
        self.is_running = False
        self.need_restart = False  # æ ‡è®°æ˜¯å¦éœ€è¦é‡å¯

        # æ—¶é—´åŒæ­¥ï¼ˆæœ¬åœ°æ—¶é—´ - æœåŠ¡å™¨æ—¶é—´çš„å·®å€¼ï¼Œå•ä½ï¼šæ¯«ç§’ï¼‰
        self.time_offset_ms = 0

        # WebSocket è¿æ¥
        self.ws_public: OkxWebSocket = None
        self.ws_business: OkxWebSocket = None

        # çº¿ç¨‹æ± ï¼ˆå‡å°å·¥ä½œçº¿ç¨‹æ•°ï¼Œé¿å…MySQLè¿æ¥æ•°è¿‡å¤šï¼‰
        self.executor = ThreadPoolExecutor(max_workers=10, thread_name_prefix="ws")
        self.executor_redis = ThreadPoolExecutor(max_workers=500, thread_name_prefix="ws")
        # 2ä¸ªWebSocketè¿æ¥çº¿ç¨‹ + æ•°æ®å¤„ç†çº¿ç¨‹ï¼ˆé¿å…Too many connectionsï¼‰

        # å†…å­˜ç¼“å†²
        self.trades_buffer = {sym: deque(maxlen=1000) for sym in self.symbols}

        # è®¢å•ç°¿åˆå§‹åŒ–æ ‡è®°
        self.orderbook_initialized = {sym: False for sym in self.symbols}

        self.kline_cache = {}
        for sym in self.symbols:
            self.kline_cache[sym] = {tf: None for tf in self.timeframes}

        # ç»Ÿè®¡
        self.stats = {
            'trades_received': 0,
            'orderbook_updates': 0,
            'klines_received': 0,
            'history_klines_loaded': 0,
            'last_update': None
        }

        # åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨
        logger.info("ğŸ“Š åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨...")
        self.data_manager = DataManager()

        # åˆå§‹åŒ–REST APIï¼ˆç”¨äºè·å–å†å²æ•°æ®ï¼‰
        if not config.is_configured():
            logger.warning("âš ï¸ æœªé…ç½®APIå¯†é’¥ï¼Œå°†æ— æ³•è·å–å†å²æ•°æ®")
            self.market_api = None
            self.public_api = None
        else:
            client = OKXRestClient(
                api_key=config.API_KEY,
                secret_key=config.SECRET_KEY,
                passphrase=config.PASSPHRASE,
                is_demo=config.IS_DEMO,
                use_proxy=config.PROXY_ENABLED,
            proxy=f'http://{config.PROXY_USERNAME}:{config.PROXY_PASSWORD}@{config.PROXY_HOST}:{config.PROXY_PORT}' if config.PROXY_USERNAME else f'http://{config.PROXY_HOST}:{config.PROXY_PORT}'
            )
            self.market_api = MarketAPI(client)
            self.public_api = PublicAPI(client)

        # è®¾ç½®ä¿¡å·å¤„ç†ï¼ˆä¼˜é›…é€€å‡ºï¼‰
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)

        logger.success("âœ“ ç‹¬ç«‹æ•°æ®é‡‡é›†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  äº¤æ˜“å¯¹: {', '.join(self.symbols)}")
        logger.info(f"  Kçº¿å‘¨æœŸ: {', '.join(self.timeframes)}")
        logger.info(f"  å†å²æ•°æ®ç­–ç•¥:")
        for tf in self.timeframes:
            days = self.HISTORY_DAYS_BY_TIMEFRAME.get(tf, self.history_days)
            logger.info(f"    {tf}: {days}å¤©")
        logger.info(f"  æ•°æ®è¶…æ—¶æ£€æµ‹: {self.data_timeout_seconds}ç§’")

    def _signal_handler(self, signum, frame):
        """å¤„ç†é€€å‡ºä¿¡å·"""
        logger.info(f"\næ”¶åˆ°é€€å‡ºä¿¡å· ({signum})ï¼Œæ­£åœ¨åœæ­¢æ•°æ®é‡‡é›†...")
        self.stop()
        sys.exit(0)

    async def _sync_server_time(self):
        """
        åŒæ­¥æœåŠ¡å™¨æ—¶é—´ï¼Œè®¡ç®—æœ¬åœ°æ—¶é—´ä¸æœåŠ¡å™¨æ—¶é—´çš„å·®å€¼

        ä½¿ç”¨å¤šæ¬¡é‡‡æ ·å–å¹³å‡å€¼ä»¥æé«˜å‡†ç¡®æ€§
        """
        try:
            if not self.public_api:
                logger.warning("âš ï¸ æœªé…ç½®APIï¼Œæ— æ³•åŒæ­¥æœåŠ¡å™¨æ—¶é—´")
                return

            logger.info("â° æ­£åœ¨åŒæ­¥æœåŠ¡å™¨æ—¶é—´...")
            samples = []

            # é‡‡æ ·3æ¬¡å–å¹³å‡å€¼
            for i in range(3):
                local_before = int(time.time() * 1000)
                result = self.public_api.get_system_time()
                local_after = int(time.time() * 1000)

                if result.get('code') == '0' and result.get('data'):
                    server_time = int(result['data'][0]['ts'])
                    # ä¼°ç®—ç½‘ç»œå»¶è¿Ÿï¼ˆå¾€è¿”æ—¶é—´çš„ä¸€åŠï¼‰
                    network_delay = (local_after - local_before) // 2
                    # æœ¬åœ°æ—¶é—´ï¼ˆå‡å»ç½‘ç»œå»¶è¿Ÿï¼‰- æœåŠ¡å™¨æ—¶é—´
                    local_time_adjusted = local_before + network_delay
                    offset = local_time_adjusted - server_time
                    samples.append(offset)

                    logger.debug(f"  é‡‡æ · {i+1}: æœ¬åœ°={local_time_adjusted}, æœåŠ¡å™¨={server_time}, å·®å€¼={offset}ms")

                if i < 2:
                    await asyncio.sleep(0.5)

            if samples:
                # å–ä¸­ä½æ•°ï¼ˆæ¯”å¹³å‡æ•°æ›´é²æ£’ï¼‰
                samples.sort()
                self.time_offset_ms = samples[len(samples) // 2]

                logger.success(
                    f"âœ“ æ—¶é—´åŒæ­¥å®Œæˆ: æœ¬åœ°æ—¶é—´æ¯”æœåŠ¡å™¨æ—¶é—´ "
                    f"{'å¿«' if self.time_offset_ms > 0 else 'æ…¢'} {abs(self.time_offset_ms)}ms"
                )
            else:
                logger.warning("âš ï¸ æ—¶é—´åŒæ­¥å¤±è´¥ï¼Œå°†ä½¿ç”¨æœ¬åœ°æ—¶é—´")

        except Exception as e:
            logger.error(f"æ—¶é—´åŒæ­¥å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()

    def get_corrected_time_ms(self) -> int:
        """
        è·å–æ ¡æ­£åçš„å½“å‰æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰

        Returns:
            æ ¡æ­£åçš„æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰= æœ¬åœ°æ—¶é—´ - æ—¶é—´å·®
        """
        local_time_ms = int(time.time() * 1000)
        return local_time_ms - self.time_offset_ms

    # ==================== å…¬å…±é¢‘é“å›è°ƒ ====================

    def on_public_open(self, ws):
        """å…¬å…±é¢‘é“è¿æ¥æ‰“å¼€å›è°ƒ"""
        # æ„å»ºè®¢é˜…å‚æ•°
        args = []
        for symbol in self.symbols:
            args.append({"channel": "books", "instId": symbol})  # 400æ¡£è®¢å•ç°¿ï¼ˆå¢é‡æ¨é€ï¼‰

        # å‘é€è®¢é˜…
        self.ws_public.subscribe(args)
        logger.info(f"âœ“ è®¢é˜…å…¬å…±é¢‘é“: {len(self.symbols)}ä¸ªäº¤æ˜“å¯¹ (trades + 400æ¡£orderbook)")

    def on_public_message(self, message: str):
        """å…¬å…±é¢‘é“æ¶ˆæ¯å›è°ƒ"""
        try:
            # è§£æ JSON æ¶ˆæ¯
            data = json.loads(message)

            # è·³è¿‡è®¢é˜…ç¡®è®¤æ¶ˆæ¯
            if data.get('event') == 'subscribe':
                logger.debug(f"è®¢é˜…æˆåŠŸ: {data}")
                return

            # å¤„ç†æ•°æ®ï¼ˆæäº¤åˆ°çº¿ç¨‹æ± ï¼Œé¿å…é˜»å¡WebSocketæ¥æ”¶ï¼‰
            if 'data' in data and 'arg' in data:
                channel = data['arg'].get('channel', '')
                inst_id = data['arg'].get('instId', '')
                action = data.get('action', '')  # snapshot / update

                if channel == 'books':
                    # å¤„ç†è®¢å•ç°¿æ•°æ®ï¼ˆå¿«ç…§æˆ–å¢é‡ï¼‰
                    for book in data['data']:
                        # æäº¤åˆ°çº¿ç¨‹æ± å¼‚æ­¥å¤„ç†
                        self._process_orderbook(inst_id, action, book)

        except Exception as e:
            logger.error(f"å…¬å…±é¢‘é“æ¶ˆæ¯å¤„ç†é”™è¯¯: {e}")

    def on_public_close(self, ws, close_code, close_msg):
        """å…¬å…±é¢‘é“è¿æ¥å…³é—­å›è°ƒ"""
        if self.is_running:
            logger.warning(f"å…¬å…±é¢‘é“è¿æ¥å…³é—­ï¼Œ5ç§’åé‡è¿...")
            time.sleep(5)
            # é‡æ–°æäº¤åˆ°çº¿ç¨‹æ± 
            if self.ws_public:
                self.executor.submit(self.ws_public.connect)

    def on_public_error(self, ws, error):
        """å…¬å…±é¢‘é“é”™è¯¯å›è°ƒ"""
        logger.error(f"å…¬å…±é¢‘é“é”™è¯¯: {error}")

    # ==================== Businessé¢‘é“å›è°ƒ ====================

    def on_business_open(self, ws):
        """Businessé¢‘é“è¿æ¥æ‰“å¼€å›è°ƒ"""
        # æ„å»ºè®¢é˜…å‚æ•°
        args = []
        for symbol in self.symbols:
            for timeframe in self.timeframes:
                channel = f"candle{timeframe}"
                args.append({"channel": channel, "instId": symbol})
            args.append(
                {
                    "channel": "trades-all",
                    "instId": symbol
                })
        # å‘é€è®¢é˜…
        self.ws_business.subscribe(args)
        logger.info(f"âœ“ è®¢é˜…businessé¢‘é“: {len(self.symbols)}ä¸ªäº¤æ˜“å¯¹ Ã— {len(self.timeframes)}ä¸ªå‘¨æœŸ")

    def on_business_message(self, message: str):
        """Businessé¢‘é“æ¶ˆæ¯å›è°ƒ"""
        try:
            # è§£æ JSON æ¶ˆæ¯
            data = json.loads(message)

            # è·³è¿‡è®¢é˜…ç¡®è®¤æ¶ˆæ¯
            if data.get('event') == 'subscribe':
                logger.debug(f"è®¢é˜…æˆåŠŸ: {data}")
                return

            # å¤„ç†Kçº¿æ•°æ®ï¼ˆæäº¤åˆ°çº¿ç¨‹æ± ï¼Œé¿å…é˜»å¡WebSocketæ¥æ”¶ï¼‰
            if 'data' in data and 'arg' in data:
                channel = data['arg'].get('channel', '')
                inst_id = data['arg'].get('instId', '')
                if channel =='trades-all':
                    self.executor_redis.submit(self._process_trade, inst_id, data['data'])

                # ä»channelä¸­æå–timeframe
                if channel.startswith('candle'):
                    timeframe = channel.replace('candle', '')
                    for kline in data['data']:
                        # æäº¤åˆ°çº¿ç¨‹æ± å¼‚æ­¥å¤„ç†
                        self.executor.submit(self._process_kline, inst_id, timeframe, kline)
            
        except Exception as e:
            logger.error(f"businessé¢‘é“æ¶ˆæ¯å¤„ç†é”™è¯¯: {e}")

    def on_business_close(self, ws, close_code, close_msg):
        """Businessé¢‘é“è¿æ¥å…³é—­å›è°ƒ"""
        if self.is_running:
            logger.warning(f"businessé¢‘é“è¿æ¥å…³é—­ï¼Œ5ç§’åé‡è¿...")
            time.sleep(5)
            # é‡æ–°æäº¤åˆ°çº¿ç¨‹æ± 
            if self.ws_business:
                self.executor.submit(self.ws_business.connect)

    def on_business_error(self, ws, error):
        """Businessé¢‘é“é”™è¯¯å›è°ƒ"""
        logger.error(f"businessé¢‘é“é”™è¯¯: {error}")

    # ==================== æ•°æ®å¤„ç†æ–¹æ³• ====================

    def _process_trade(self, symbol: str, trades: dict):
        """å¤„ç†é€ç¬”æˆäº¤ï¼ˆä»…ä¿å­˜åˆ°Redisï¼ŒAIåˆ†ææ—¶å†èšåˆï¼‰"""
        try:
            trade_data = [{
                'trade_id': trade['tradeId'],
                'timestamp': int(trade['ts']),
                'price': float(trade['px']),
                'size': float(trade['sz']),
                'side': trade['side']
            } for trade in trades]

            # æ›´æ–°ç»Ÿè®¡
            self.stats['trades_received'] += len(trade_data)
            self.stats['last_update'] = datetime.now()

            # å®æ—¶å†™å…¥Redisï¼ˆä¾›AIåˆ†ææ—¶æŸ¥è¯¢å’Œèšåˆï¼‰
            self.data_manager.save_trades_to_redis(symbol, trade_data)

        except Exception as e:
            logger.error(f"å¤„ç†æˆäº¤æ•°æ®å¤±è´¥: {e}")

    def _process_orderbook(self, symbol: str, action: str, book: dict):
        """
        å¤„ç†è®¢å•ç°¿æ•°æ®ï¼ˆå¿«ç…§ + å¢é‡æ›´æ–°ï¼‰

        Args:
            symbol: äº¤æ˜“å¯¹
            action: 'snapshot' (å…¨é‡å¿«ç…§) æˆ– 'update' (å¢é‡æ›´æ–°)
            book: è®¢å•ç°¿æ•°æ® {bids, asks, ts, checksum, seqId, prevSeqId}

        OKXè®¢å•ç°¿æ›´æ–°æœºåˆ¶ï¼š
        1. é¦–æ¬¡æ¨é€ï¼šaction='snapshot', prevSeqId=-1
           - æ¸…ç©ºæ•°æ®åº“ï¼Œä¿å­˜å…¨é‡400æ¡£æ•°æ®
        2. åç»­æ¨é€ï¼šaction='update'
           - å¢é‡æ›´æ–°ï¼šsize='0'åˆ é™¤ï¼Œsize!='0'æ›´æ–°æˆ–æ’å…¥
        3. æ ¡éªŒï¼šä½¿ç”¨checksuméªŒè¯æ•°æ®å®Œæ•´æ€§
        """
        try:
            bids = book.get('bids', [])
            asks = book.get('asks', [])
            timestamp = int(book['ts'])
            checksum = book.get('checksum')
            seqId = book.get('seqId', -1)
            prevSeqId = book.get('prevSeqId', -1)

            if not bids and not asks:
                # ç©ºæ›´æ–°ï¼ˆç»´æŒå¿ƒè·³ï¼‰ï¼ŒprevSeqId == seqId
                logger.debug(f"è®¢å•ç°¿å¿ƒè·³: {symbol}, seqId={seqId}")
                return

            # æ£€æŸ¥åºåˆ—å·å¼‚å¸¸ï¼ˆåºåˆ—é‡ç½®ï¼‰
            if action == 'update' and seqId < prevSeqId:
                logger.warning(f"âš ï¸  è®¢å•ç°¿åºåˆ—é‡ç½®: {symbol}, prevSeqId={prevSeqId} -> seqId={seqId}")
                # åºåˆ—é‡ç½®åï¼Œåç»­æ¶ˆæ¯ä¼šéµå¾ªæ­£å¸¸æ’åº

            # å¤„ç†å¿«ç…§æˆ–å¢é‡
            if action == 'snapshot':
                # å…¨é‡å¿«ç…§ - ä¿å­˜åˆ°Redis
                self.data_manager.save_orderbook_to_redis(
                    symbol=symbol,
                    action=action,
                    bids=bids,
                    asks=asks,
                    timestamp=timestamp,
                    seqId=seqId
                )
                self.orderbook_initialized[symbol] = True

            elif action == 'update':
                # å¢é‡æ›´æ–° - ä¿å­˜åˆ°Redis
                if not self.orderbook_initialized.get(symbol, False):
                    logger.warning(f"âš ï¸  è®¢å•ç°¿æœªåˆå§‹åŒ–ï¼Œè·³è¿‡å¢é‡æ›´æ–°: {symbol}")
                    return

                self.data_manager.save_orderbook_to_redis(
                    symbol=symbol,
                    action=action,
                    bids=bids,
                    asks=asks,
                    timestamp=timestamp,
                    seqId=seqId
                )



            # å®šæœŸä¿å­˜èšåˆæŒ‡æ ‡åˆ°SQLiteï¼ˆç”¨äºå¿«é€ŸæŸ¥è¯¢å’Œå†å²åˆ†æï¼‰
            if self.stats['orderbook_updates'] % 10 == 0:
                self._save_orderbook_snapshot_metrics(symbol)

        except Exception as e:
            logger.error(f"å¤„ç†è®¢å•ç°¿å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

    def _save_orderbook_snapshot_metrics(self, symbol: str):
        """
        ä¿å­˜è®¢å•ç°¿èšåˆæŒ‡æ ‡ï¼ˆå®šæœŸå¿«ç…§ï¼Œç”¨äºå¿«é€ŸæŸ¥è¯¢ï¼‰

        ä»orderbook_liveæå–å‰5æ¡£æ•°æ®ï¼Œè®¡ç®—èšåˆæŒ‡æ ‡
        """
        try:
            orderbook = self.data_manager.get_orderbook_live(symbol, depth=5)
            if not orderbook or not orderbook['bids'] or not orderbook['asks']:
                return

            bids = orderbook['bids']
            asks = orderbook['asks']
            timestamp = orderbook['timestamp']

            # æå–ç¬¬1æ¡£
            bid1_price = float(bids[0][0])
            bid1_size = float(bids[0][1])
            ask1_price = float(asks[0][0])
            ask1_size = float(asks[0][1])

            # è®¡ç®—ä¸­é—´ä»·å’Œä»·å·®
            mid_price = (bid1_price + ask1_price) / 2
            spread_pct = (ask1_price - bid1_price) / mid_price * 100

            # è®¡ç®—5æ¡£æ·±åº¦
            bid_depth_5 = sum(float(b[1]) for b in bids[:5])
            ask_depth_5 = sum(float(a[1]) for a in asks[:5])
            depth_ratio = bid_depth_5 / ask_depth_5 if ask_depth_5 > 0 else 0

            # ä¿å­˜èšåˆæŒ‡æ ‡
            snapshot_data = {
                'timestamp': timestamp,
                'bid1_price': bid1_price,
                'bid1_size': bid1_size,
                'ask1_price': ask1_price,
                'ask1_size': ask1_size,
                'spread_pct': spread_pct,
                'bid_depth_5': bid_depth_5,
                'ask_depth_5': ask_depth_5,
                'depth_ratio': depth_ratio
            }

            self.data_manager.save_orderbook_snapshot(symbol, snapshot_data)

        except Exception as e:
            logger.error(f"ä¿å­˜è®¢å•ç°¿æŒ‡æ ‡å¤±è´¥: {e}")

    def _process_kline(self, symbol: str, timeframe: str, kline: list):
        """
        å¤„ç†Kçº¿æ•°æ®ï¼ˆåŒ…æ‹¬æœªå®Œç»“Kçº¿çš„å®æ—¶æ›´æ–°ï¼‰

        é‡è¦é€»è¾‘ï¼š
        1. confirm="0" (æœªå®Œç»“): æŒç»­UPDATEæ•°æ®åº“çš„high/low/close/volumeï¼Œis_confirmed=0
        2. confirm="1" (å®Œç»“): æœ€åä¸€æ¬¡UPDATEï¼ŒåŒ…æ‹¬æœ€ç»ˆä»·æ ¼å’Œis_confirmed=1

        OKX WebSocketæ¨é€é¢‘ç‡ï¼š
        - æœªå®Œç»“Kçº¿ï¼šä»·æ ¼å˜åŒ–æ—¶æ¨é€ï¼ˆé«˜é¢‘ï¼‰
        - å®Œç»“Kçº¿ï¼šå‘¨æœŸç»“æŸæ—¶æ¨é€ä¸€æ¬¡ï¼ˆconfirm="1"ï¼‰

        æ•°æ®åº“æ“ä½œï¼š
        - ä½¿ç”¨ INSERT OR REPLACE å®ç°UPSERT
        - åŒä¸€æ—¶é—´æˆ³çš„Kçº¿ä¼šä¸æ–­è¢«æ›´æ–°ï¼Œç›´åˆ°å®Œç»“
        """
        try:
            confirm = kline[8] if len(kline) > 8 else "0"
            is_confirmed = (confirm == "1")

            # è§£æKçº¿æ•°æ®
            kline_data = {
                'timestamp': int(kline[0]),
                'open': float(kline[1]),
                'high': float(kline[2]),
                'low': float(kline[3]),
                'close': float(kline[4]),
                'volume': float(kline[5]),
                'confirm': confirm,
                'is_confirmed': is_confirmed
            }

            # ç¼“å­˜åˆ°å†…å­˜ï¼ˆå§‹ç»ˆä¿æŒæœ€æ–°çŠ¶æ€ï¼‰
            self.kline_cache[symbol][timeframe] = kline_data

            # ä¿å­˜/æ›´æ–°åˆ°æ•°æ®åº“ï¼ˆæ¯æ¬¡æ¨é€éƒ½ä¼šæ›´æ–°ï¼‰
            # - æœªå®Œç»“æ—¶ï¼šä¸æ–­UPDATEæœ€æ–°çš„ä»·æ ¼æ•°æ®
            # - å®Œç»“æ—¶ï¼šæœ€åä¸€æ¬¡UPDATEå¹¶æ ‡è®°is_confirmed=1
            self.data_manager.save_kline(
                inst_id=symbol,
                bar=timeframe,
                kline_data=kline
            )

            # è®°å½•Kçº¿æ•°æ®çš„æœ€åæ›´æ–°æ—¶é—´ï¼ˆå®é™…æ¥æ”¶åˆ°æ•°æ®çš„æ—¶é—´ï¼‰
            self.data_manager.update_kline_last_update(symbol, timeframe)

            self.stats['klines_received'] += 1

            # æ—¥å¿—è¾“å‡ºï¼ˆåŒºåˆ†å®Œç»“å’Œæœªå®Œç»“ï¼‰
            if is_confirmed:
                logger.debug(
                    f"âœ“ Kçº¿å®Œç»“: {symbol} {timeframe} | "
                    f"O={kline_data['open']:.2f} H={kline_data['high']:.2f} "
                    f"L={kline_data['low']:.2f} C={kline_data['close']:.2f} "
                    f"V={kline_data['volume']:.2f}"
                )
            else:
                # æœªå®Œç»“Kçº¿ä¹Ÿè¾“å‡ºï¼Œä½†é¢‘ç‡è¾ƒä½ï¼ˆé¿å…åˆ·å±ï¼‰
                if self.stats['klines_received'] % 10 == 0:  # æ¯10æ¬¡æ›´æ–°è¾“å‡º1æ¬¡
                    logger.debug(
                        f"âŸ³ Kçº¿æ›´æ–°: {symbol} {timeframe} | "
                        f"å½“å‰ä»·={kline_data['close']:.2f} "
                        f"H={kline_data['high']:.2f} L={kline_data['low']:.2f} "
                        f"[æœªå®Œç»“-æŒç»­æ›´æ–°ä¸­]"
                    )

        except Exception as e:
            logger.error(f"å¤„ç†Kçº¿æ•°æ®å¤±è´¥: {e}")

    async def start(self):
        """å¯åŠ¨æ•°æ®é‡‡é›†"""
        if self.is_running:
            logger.warning("âš ï¸ æ•°æ®é‡‡é›†å™¨å·²åœ¨è¿è¡Œä¸­")
            return

        logger.info("=" * 60)
        logger.info("ğŸš€ å¯åŠ¨ç‹¬ç«‹æ•°æ®é‡‡é›†å™¨")
        logger.info(f"å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("=" * 60)

        self.is_running = True

        try:
            # 0. åŒæ­¥æœåŠ¡å™¨æ—¶é—´ï¼ˆé‡è¦ï¼ç”¨äºæ ¡æ­£æ•°æ®æ›´æ–°æ—¶é—´ï¼‰
            await self._sync_server_time()

            # 1. æ¸…ç©ºRedisä¸­çš„æ—§æ•°æ®ï¼ˆé‡è¦ï¼ï¼‰
            logger.info("ğŸ§¹ æ¸…ç©ºRedisæ—§æ•°æ®...")
            for symbol in self.symbols:
                # æ¸…ç©ºè®¢å•ç°¿
                self.data_manager.clear_redis_orderbook(symbol)
                # æ¸…ç©ºé€ç¬”æˆäº¤
                self.data_manager.clear_redis_trades(symbol)
            logger.success("âœ“ Redisæ•°æ®å·²æ¸…ç©ºï¼ˆè®¢å•ç°¿ + é€ç¬”æˆäº¤ï¼‰")

            # 1. ä¿®å¤æœªå®Œç»“çš„å†å²Kçº¿ï¼ˆé‡è¦ï¼ï¼‰
            if self.market_api:
                logger.info("ğŸ”§ æ£€æŸ¥å¹¶ä¿®å¤æœªå®Œç»“çš„å†å²Kçº¿...")
                asyncio.create_task(self._fix_unconfirmed_klines())
            else:
                logger.warning("âš ï¸ æœªæä¾›MarketAPIï¼Œè·³è¿‡æœªå®Œç»“Kçº¿ä¿®å¤")

            # 2. å†å²æ•°æ®åˆå§‹åŒ–ï¼ˆåå°ä»»åŠ¡ï¼‰
            if self.market_api:
                logger.info("ğŸ“š å†å²æ•°æ®åˆå§‹åŒ–å°†åœ¨åå°è¿è¡Œ...")
                asyncio.create_task(self._init_history_data())
            else:
                logger.warning("âš ï¸ æœªæä¾›MarketAPIï¼Œè·³è¿‡å†å²æ•°æ®åˆå§‹åŒ–")

            # 3. åˆ›å»º WebSocket è¿æ¥
            self.ws_public = OkxWebSocket(
                url="wss://ws.okx.com:8443/ws/v5/public",
                name="å…¬å…±é¢‘é“",
                on_open=self.on_public_open,
                on_message=self.on_public_message,
                on_close=self.on_public_close,
                on_error=self.on_public_error,
                ping_interval=10,
                proxy=f'http://{config.PROXY_USERNAME}:{config.PROXY_PASSWORD}@{config.PROXY_HOST}:{config.PROXY_PORT}' if config.PROXY_USERNAME else f'http://{config.PROXY_HOST}:{config.PROXY_PORT}' ,
                use_proxy=config.PROXY_ENABLED,
            )

            self.ws_business = OkxWebSocket(
                url="wss://ws.okx.com:8443/ws/v5/business",
                name="businessé¢‘é“",
                on_open=self.on_business_open,
                on_message=self.on_business_message,
                on_close=self.on_business_close,
                on_error=self.on_business_error,
                ping_interval=20,
                proxy=f'http://{config.PROXY_USERNAME}:{config.PROXY_PASSWORD}@{config.PROXY_HOST}:{config.PROXY_PORT}' if config.PROXY_USERNAME else f'http://{config.PROXY_HOST}:{config.PROXY_PORT}' ,
                use_proxy=config.PROXY_ENABLED,
            )

            # 4. åœ¨çº¿ç¨‹æ± ä¸­å¯åŠ¨ WebSocket è¿æ¥
            logger.info("ğŸ”Œ å¯åŠ¨ WebSocket è¿æ¥...")
            future_public = self.executor.submit(self.ws_public.connect)
            future_business = self.executor.submit(self.ws_business.connect)

            logger.success("âœ“ WebSocketè¿æ¥å·²åœ¨çº¿ç¨‹æ± ä¸­å¯åŠ¨")
            logger.info("ğŸ“¡ æ­£åœ¨æ¥æ”¶å®æ—¶æ•°æ®...")
            logger.info("ğŸ’¡ æŒ‰ Ctrl+C åœæ­¢é‡‡é›†")

            # 5. å¯åŠ¨åå°ä»»åŠ¡
            tasks = [
                #asyncio.create_task(self._aggregate_pressure_loop()),
                asyncio.create_task(self._snapshot_orderbook_loop()),
                asyncio.create_task(self._cleanup_old_data_loop()),
                asyncio.create_task(self._stats_report_loop()),
                asyncio.create_task(self._monitor_status())
            ]

            # ä¿å­˜åˆ°å®ä¾‹å˜é‡ï¼Œä»¥ä¾¿åœ¨éœ€è¦é‡å¯æ—¶å¯ä»¥å–æ¶ˆ
            self.background_tasks = tasks

            try:
                await asyncio.gather(*tasks, return_exceptions=True)
            except asyncio.CancelledError:
                logger.info("åå°ä»»åŠ¡è¢«å–æ¶ˆ")
                # å–æ¶ˆæ‰€æœ‰æœªå®Œæˆçš„ä»»åŠ¡
                for task in tasks:
                    if not task.done():
                        task.cancel()
                # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆå–æ¶ˆ
                await asyncio.gather(*tasks, return_exceptions=True)

        except Exception as e:
            logger.error(f"âŒ æ•°æ®é‡‡é›†å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
        finally:
            self.stop()

    # ==================== åå°ä»»åŠ¡ ====================

    async def _fix_unconfirmed_klines(self):
        """
        ä¿®å¤æœªå®Œç»“çš„å†å²Kçº¿ï¼ˆå¯åŠ¨æ—¶æ‰§è¡Œï¼‰

        å½“é‡‡é›†å™¨åœæ­¢åå†å¯åŠ¨æ—¶ï¼Œæ•°æ®åº“ä¸­å¯èƒ½å­˜åœ¨ is_confirmed=0 ä½†å®é™…å·²å®Œç»“çš„Kçº¿
        æ­¤æ–¹æ³•ä¼šï¼š
        1. æŸ¥æ‰¾æ‰€æœ‰æœªå®Œç»“çš„Kçº¿
        2. åˆ¤æ–­æ˜¯å¦å·²è¿‡æœŸï¼ˆå½“å‰æ—¶é—´ > Kçº¿æ—¶é—´ + å‘¨æœŸï¼‰
        3. è°ƒç”¨REST APIè·å–æ­£ç¡®çš„å†å²æ•°æ®
        4. æ›´æ–°æ•°æ®åº“ï¼Œè®¾ç½® is_confirmed=1
        """
        try:
            await asyncio.sleep(1)  # ç­‰å¾…1ç§’ç¡®ä¿æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ

            # è·å–æ‰€æœ‰æœªå®Œç»“çš„Kçº¿
            unconfirmed = self.data_manager.get_unconfirmed_klines()

            if not unconfirmed:
                logger.info("âœ“ æ•°æ®åº“ä¸­æ— æœªå®Œç»“Kçº¿ï¼Œæ— éœ€ä¿®å¤")
                return

            logger.info(f"ğŸ” å‘ç° {len(unconfirmed)} æ ¹æœªå®Œç»“Kçº¿ï¼Œå¼€å§‹ä¿®å¤...")

            now_ms = int(time.time() * 1000)
            fixed_count = 0
            skipped_count = 0

            # æŒ‰äº¤æ˜“å¯¹å’Œå‘¨æœŸåˆ†ç»„
            from collections import defaultdict
            groups = defaultdict(list)
            for kline in unconfirmed:
                key = (kline['inst_id'], kline['bar'])
                groups[key].append(kline)

            # é€ä¸ªå¤„ç†
            for (inst_id, bar), klines in groups.items():
                logger.info(f"  å¤„ç† {inst_id} {bar}: {len(klines)} æ ¹Kçº¿")

                bar_duration_ms = self.BAR_TO_MS.get(bar, 60000)  # é»˜è®¤1åˆ†é’Ÿ

                for kline in klines:
                    timestamp = kline['timestamp']
                    kline_end_time = timestamp + bar_duration_ms

                    # åˆ¤æ–­æ˜¯å¦å·²è¿‡æœŸï¼ˆå·²å®Œç»“ï¼‰
                    if now_ms <= kline_end_time:
                        # æœªè¿‡æœŸï¼Œè·³è¿‡ï¼ˆå¯èƒ½æ˜¯åˆšå¯åŠ¨æ—¶çš„æœ€æ–°Kçº¿ï¼‰
                        skipped_count += 1
                        continue

                    # å·²è¿‡æœŸï¼Œéœ€è¦ä¿®å¤
                    try:
                        # è°ƒç”¨REST APIè·å–è¯¥æ—¶é—´ç‚¹çš„å†å²Kçº¿
                        # æ³¨æ„ï¼šOKX APIçš„ after/before å‚æ•°æ˜¯æŒ‰æ—¶é—´å€’åºçš„
                        result = self.market_api.get_history_candles(
                            inst_id=inst_id,
                            bar=bar,
                            after=str(timestamp),
                            limit='1'
                        )

                        if result['code'] != '0':
                            logger.warning(f"    âš ï¸  APIè°ƒç”¨å¤±è´¥: {result.get('msg')}")
                            continue

                        candles = result.get('data', [])
                        if not candles:
                            logger.warning(f"    âš ï¸  æœªæ‰¾åˆ°å†å²æ•°æ®: timestamp={timestamp}")
                            continue

                        # æ‰¾åˆ°åŒ¹é…æ—¶é—´æˆ³çš„Kçº¿
                        matched = None
                        for candle in candles:
                            if int(candle[0]) == timestamp:
                                matched = candle
                                break

                        if not matched:
                            # å¦‚æœæ²¡æœ‰ç²¾ç¡®åŒ¹é…ï¼Œä½¿ç”¨æœ€æ¥è¿‘çš„
                            matched = candles[0]

                        # æ›´æ–°æ•°æ®åº“
                        self.data_manager.update_kline_confirmed(
                            inst_id=inst_id,
                            bar=bar,
                            timestamp=timestamp,
                            open_price=float(matched[1]),
                            high=float(matched[2]),
                            low=float(matched[3]),
                            close=float(matched[4]),
                            volume=float(matched[5])
                        )

                        fixed_count += 1

                        # é¿å…è¯·æ±‚è¿‡å¿«
                        await asyncio.sleep(0.1)

                    except Exception as e:
                        logger.error(f"    âŒ ä¿®å¤Kçº¿å¤±è´¥: {e}")

            logger.success(
                f"âœ“ Kçº¿ä¿®å¤å®Œæˆ: ä¿®å¤ {fixed_count} æ ¹, è·³è¿‡ {skipped_count} æ ¹ï¼ˆæœªè¿‡æœŸï¼‰"
            )

        except Exception as e:
            logger.error(f"âŒ ä¿®å¤æœªå®Œç»“Kçº¿å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()

    async def _init_history_data(self):
        """åˆå§‹åŒ–å†å²æ•°æ®ï¼ˆåå°è¿è¡Œï¼‰"""
        from datetime import timedelta

        try:
            await asyncio.sleep(2)  # ç­‰å¾…2ç§’è®©å®æ—¶é‡‡é›†å…ˆå¯åŠ¨
            logger.info(f"ğŸ“š å¼€å§‹åˆå§‹åŒ–å†å²æ•°æ®ï¼ˆä¸åŒå‘¨æœŸä½¿ç”¨ä¸åŒå¤©æ•°ï¼‰...")
            total_loaded = 0

            for symbol in self.symbols:
                for timeframe in self.timeframes:
                    try:
                        # è·å–è¯¥timeframeå¯¹åº”çš„å†å²å¤©æ•°
                        history_days = self.HISTORY_DAYS_BY_TIMEFRAME.get(timeframe, self.history_days)
                        logger.info(f"  æ£€æŸ¥ {symbol} {timeframe} (æœ€è¿‘{history_days}å¤©)...")

                        # è·å–éœ€è¦è¡¥é½çš„æ—¶é—´åŒºé—´åˆ—è¡¨
                        missing_ranges = await self._detect_missing_ranges(symbol, timeframe)

                        if not missing_ranges:
                            logger.info(f"  {symbol} {timeframe}: âœ“ æ•°æ®å®Œæ•´ï¼Œæ— éœ€è¡¥é½")
                            continue

                        # è®¡ç®—ç¼ºå¤±çš„Kçº¿æ€»æ•°
                        bar_interval_ms = self.BAR_TO_MS.get(timeframe, 60 * 1000)
                        total_missing_bars = sum(
                            (end_ts - start_ts) // bar_interval_ms
                            for start_ts, end_ts in missing_ranges
                        )

                        logger.info(
                            f"  {symbol} {timeframe}: å‘ç° {len(missing_ranges)} ä¸ªç¼ºå¤±åŒºé—´ï¼Œ"
                            f"å…±ç¼ºå¤± {total_missing_bars} æ ¹Kçº¿"
                        )

                        # è¡¥é½æ¯ä¸ªç¼ºå¤±åŒºé—´
                        for i, (start_ts, end_ts) in enumerate(missing_ranges, 1):
                            start_date = datetime.fromtimestamp(start_ts / 1000).strftime('%Y-%m-%d %H:%M:%S')
                            end_date = datetime.fromtimestamp(end_ts / 1000).strftime('%Y-%m-%d %H:%M:%S')
                            bars_in_range = (end_ts - start_ts) // bar_interval_ms

                            logger.info(
                                f"    è¡¥é½åŒºé—´ {i}/{len(missing_ranges)}: "
                                f"{start_date} è‡³ {end_date} (çº¦ {bars_in_range} æ ¹Kçº¿)"
                            )
                            bl=tqdm(total=bars_in_range,desc=f'ä¸‹è½½Kçº¿æ•°æ®ã€{timeframe}ã€‘')

                            # OKX API: after=Tè¿”å›<Tçš„æ•°æ®, before=Tè¿”å›>Tçš„æ•°æ®
                            # Kçº¿æ—¶é—´æˆ³æ˜¯å‘¨æœŸå¼€å§‹æ—¶é—´ï¼Œæ‰€ä»¥éœ€è¦è®©beforeå‚æ•°å¾€å‰ç§»ä¸€ç‚¹
                            # ä½¿ç”¨ before=start_ts-1 æ¥åŒ…å« start_ts é‚£æ ¹Kçº¿
                            count = await self._fetch_and_save_klines(
                                symbol, timeframe,
                                after=str(end_ts),
                                before=str(start_ts - 1) if start_ts > 0 else None,
                                bl=bl
                            )

                            total_loaded += count
                            logger.success(f"      âœ“ åŠ è½½ {count} æ ¹Kçº¿")

                            # é¿å…è¯·æ±‚è¿‡å¿«
                            await asyncio.sleep(0.5)

                    except Exception as e:
                        logger.error(f"  âŒ {symbol} {timeframe} å†å²æ•°æ®åŠ è½½å¤±è´¥: {e}")
                        import traceback
                        traceback.print_exc()

            self.stats['history_klines_loaded'] = total_loaded
            logger.success(f"âœ“ å†å²æ•°æ®åˆå§‹åŒ–å®Œæˆï¼Œå…±åŠ è½½ {total_loaded} æ ¹Kçº¿")

        except Exception as e:
            logger.error(f"åå°å†å²æ•°æ®åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

    async def _detect_missing_ranges(self, symbol: str, timeframe: str) -> list:
        """
        æ£€æµ‹ç¼ºå¤±çš„Kçº¿æ—¶é—´åŒºé—´ï¼ˆæŒ‰timeframeå‘¨æœŸæ£€æµ‹ï¼‰

        Args:
            symbol: äº¤æ˜“å¯¹
            timeframe: Kçº¿å‘¨æœŸ

        Returns:
            ç¼ºå¤±åŒºé—´åˆ—è¡¨ [(start_ts, end_ts), ...]
        """
        from datetime import timedelta

        bar_interval_ms = self.BAR_TO_MS.get(timeframe, 60 * 1000)

        # æ ¹æ®timeframeè·å–å¯¹åº”çš„å†å²å¤©æ•°
        history_days = self.HISTORY_DAYS_BY_TIMEFRAME.get(timeframe, self.history_days)

        # è®¡ç®—åº”è¯¥æœ‰æ•°æ®çš„æ—¶é—´èŒƒå›´
        now = datetime.now()
        end_time = now
        start_time = now - timedelta(days=history_days)

        # å¯¹é½åˆ°Kçº¿å‘¨æœŸçš„èµ·å§‹ç‚¹ï¼ˆå‘ä¸‹å–æ•´ï¼‰
        start_time_ms = int(start_time.timestamp() * 1000)
        start_time_ms = (start_time_ms // bar_interval_ms) * bar_interval_ms

        end_time_ms = int(end_time.timestamp() * 1000)
        end_time_ms = (end_time_ms // bar_interval_ms) * bar_interval_ms

        # è·å–æ•°æ®åº“ä¸­çš„æ‰€æœ‰Kçº¿æ•°æ®
        # è®¡ç®—éœ€è¦æŸ¥è¯¢çš„æ•°é‡ï¼ˆæŒ‰timeframeè®¡ç®—ï¼‰
        total_bars = (end_time_ms - start_time_ms) // bar_interval_ms
        limit = min(total_bars + 100, 50000)  # æœ€å¤šæŸ¥è¯¢5ä¸‡æ ¹Kçº¿

        existing_klines = self.data_manager.get_recent_klines(
            symbol, timeframe,
            limit=int(limit)
        )

        if not existing_klines:
            # æ•°æ®åº“ä¸­æ— æ•°æ®ï¼Œæ•´ä¸ªæ—¶é—´èŒƒå›´éƒ½ç¼ºå¤±
            return [(start_time_ms, end_time_ms)]

        # å°†ç°æœ‰æ•°æ®çš„æ—¶é—´æˆ³è½¬æ¢ä¸ºé›†åˆ
        existing_timestamps = {kline['timestamp'] for kline in existing_klines}

        # ç”Ÿæˆåº”è¯¥å­˜åœ¨çš„æ‰€æœ‰Kçº¿æ—¶é—´æˆ³
        expected_timestamps = []
        current_ts = start_time_ms
        while current_ts <= end_time_ms:
            expected_timestamps.append(current_ts)
            current_ts += bar_interval_ms

        # æ‰¾å‡ºç¼ºå¤±çš„æ—¶é—´æˆ³
        missing_timestamps = [ts for ts in expected_timestamps if ts not in existing_timestamps]

        if not missing_timestamps:
            # æ²¡æœ‰ç¼ºå¤±æ•°æ®
            return []

        # å°†è¿ç»­ç¼ºå¤±çš„æ—¶é—´æˆ³åˆå¹¶ä¸ºåŒºé—´
        missing_ranges = []
        range_start = missing_timestamps[0]
        range_end = missing_timestamps[0]

        for i in range(1, len(missing_timestamps)):
            if missing_timestamps[i] == range_end + bar_interval_ms:
                # è¿ç»­ç¼ºå¤±ï¼Œæ‰©å±•å½“å‰åŒºé—´
                range_end = missing_timestamps[i]
            else:
                # ä¸è¿ç»­ï¼Œä¿å­˜å½“å‰åŒºé—´ï¼Œå¼€å§‹æ–°åŒºé—´
                missing_ranges.append((range_start, range_end + bar_interval_ms))
                range_start = missing_timestamps[i]
                range_end = missing_timestamps[i]

        # ä¿å­˜æœ€åä¸€ä¸ªåŒºé—´
        missing_ranges.append((range_start, range_end + bar_interval_ms))

        return missing_ranges

    async def _fetch_and_save_klines(self, symbol: str, timeframe: str, after: str = None, before: str = None,bl:tqdm = None) -> int:
        """è·å–å¹¶ä¿å­˜Kçº¿æ•°æ®ï¼ˆæ‰¹é‡ä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        count = 0
        current_after = after
        max_iterations = 1000

        for iteration in range(max_iterations):
            try:
                result = self.market_api.get_history_candles(
                    inst_id=symbol,
                    bar=timeframe,
                    after=current_after,
                    before=before,
                    limit='100'
                )

                if result['code'] != '0':
                    logger.error(f"è·å–Kçº¿å¤±è´¥: {result.get('msg')}")
                    break

                klines = result.get('data', [])
                if not klines:
                    break

                # æ‰¹é‡ä¿å­˜Kçº¿ï¼ˆä¸€æ¬¡æ€§ä¿å­˜æ‰€æœ‰Kçº¿ï¼Œå¤§å¹…æå‡æ•ˆç‡ï¼‰
                saved = self.data_manager.save_klines_batch(
                    inst_id=symbol,
                    bar=timeframe,
                    klines_data=klines
                )
                count += saved

                # æ›´æ–°è¿›åº¦æ¡
                if bl:
                    bl.update(saved)

                # è·å–æœ€æ—§çš„æ—¶é—´æˆ³
                oldest_ts = klines[-1][0]

                if current_after and oldest_ts == current_after:
                    break

                current_after = oldest_ts

                if len(klines) < 100:
                    break


            except Exception as e:
                logger.error(f"è·å–Kçº¿æ•°æ®å¼‚å¸¸: {e}")
                break

        return count

    async def _aggregate_pressure_loop(self):
        """èšåˆå¸‚åœºå‹åŠ›æŒ‡æ ‡ï¼ˆæ¯åˆ†é’Ÿ1æ¬¡ï¼‰"""
        while self.is_running:
            try:
                await asyncio.sleep(60)

                for symbol in self.symbols:
                    for interval_sec in [60, 300, 900]:
                        trades = self.data_manager.get_recent_trades(symbol, interval_sec)
                        if trades:
                            # è®¡ç®—å‹åŠ›æŒ‡æ ‡
                            buy_volume = sum(t['size'] for t in trades if t['side'] == 'buy')
                            sell_volume = sum(t['size'] for t in trades if t['side'] == 'sell')
                            buy_count = sum(1 for t in trades if t['side'] == 'buy')
                            sell_count = sum(1 for t in trades if t['side'] == 'sell')
                            pressure_ratio = buy_volume / sell_volume if sell_volume > 0 else 0

                            pressure = {
                                'timestamp': int(time.time() * 1000),
                                'buy_volume': buy_volume,
                                'sell_volume': sell_volume,
                                'buy_count': buy_count,
                                'sell_count': sell_count,
                                'pressure_ratio': pressure_ratio
                            }
                            self.data_manager.save_market_pressure(symbol, interval_sec, pressure)

                logger.debug("âœ“ å¸‚åœºå‹åŠ›æŒ‡æ ‡å·²æ›´æ–°")

            except Exception as e:
                logger.error(f"å‹åŠ›èšåˆé”™è¯¯: {e}")

    async def _snapshot_orderbook_loop(self):
        """å®šæœŸä»æ•°æ®åº“æŸ¥è¯¢è®¢å•ç°¿å¹¶ä¿å­˜å¿«ç…§æŒ‡æ ‡ï¼ˆæ¯åˆ†é’Ÿ1æ¬¡ï¼‰"""
        while self.is_running:
            try:
                await asyncio.sleep(60)

                for symbol in self.symbols:
                    # ä»æ•°æ®åº“è·å–æœ€æ–°è®¢å•ç°¿
                    self._save_orderbook_snapshot_metrics(symbol)

                logger.debug("âœ“ è®¢å•ç°¿å¿«ç…§å·²ä¿å­˜")

            except Exception as e:
                logger.error(f"è®¢å•ç°¿å¿«ç…§é”™è¯¯: {e}")

    async def _cleanup_old_data_loop(self):
        """æ¸…ç†æ—§æ•°æ®ï¼ˆæ¯å°æ—¶1æ¬¡ï¼‰"""
        while self.is_running:
            try:
                await asyncio.sleep(3600)

                # åªä¿ç•™æœ€è¿‘1å°æ—¶çš„é€ç¬”æˆäº¤
                self.data_manager.cleanup_old_trades(hours=1)

                # åªä¿ç•™æœ€è¿‘24å°æ—¶çš„è®¢å•ç°¿å¿«ç…§
                self.data_manager.cleanup_old_orderbook(hours=24)

                # åªä¿ç•™æœ€è¿‘24å°æ—¶çš„åŸå§‹è®¢å•ç°¿æ•°æ®
                self.data_manager.cleanup_old_orderbook_raw(hours=24)

                logger.info("âœ“ æ—§æ•°æ®å·²æ¸…ç†")

            except Exception as e:
                logger.error(f"æ¸…ç†æ•°æ®é”™è¯¯: {e}")

    async def _stats_report_loop(self):
        """ç»Ÿè®¡æŠ¥å‘Šï¼ˆæ¯5åˆ†é’Ÿï¼‰"""
        while self.is_running:
            try:
                await asyncio.sleep(300)

                logger.info(
                    f"ğŸ“Š é‡‡é›†ç»Ÿè®¡ | "
                    f"å†å²Kçº¿: {self.stats['history_klines_loaded']}æ ¹ | "
                    f"å®æ—¶æˆäº¤: {self.stats['trades_received']}ç¬” | "
                    f"è®¢å•ç°¿: {self.stats['orderbook_updates']}æ¬¡ | "
                    f"å®æ—¶Kçº¿: {self.stats['klines_received']}æ ¹ | "
                    f"æœ€åæ›´æ–°: {self.stats['last_update'].strftime('%H:%M:%S') if self.stats['last_update'] else 'N/A'}"
                )

            except Exception as e:
                logger.error(f"ç»Ÿè®¡æŠ¥å‘Šé”™è¯¯: {e}")

    async def _monitor_status(self):
        """ç›‘æ§è¿è¡ŒçŠ¶æ€å¹¶å®šæœŸæ‰“å°ç»Ÿè®¡ä¿¡æ¯ï¼Œæ£€æµ‹æ•°æ®æ›´æ–°è¶…æ—¶"""
        status_interval = 30  # æ¯60ç§’æ‰“å°ä¸€æ¬¡çŠ¶æ€
        await asyncio.sleep(30)
        while self.is_running:
            try:
                await asyncio.sleep(status_interval)

                # è·å–å½“å‰æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼Œä½¿ç”¨æ ¡æ­£åçš„æœåŠ¡å™¨æ—¶é—´ï¼‰
                now_ms = self.get_corrected_time_ms()
                timeout_threshold_ms = self.data_timeout_seconds * 1000

                # æ£€æŸ¥å„ä¸ªæ•°æ®æºçš„æ›´æ–°è¶…æ—¶
                timeout_detected = False
                timeout_details = []

                for symbol in self.symbols:
                    # 1. æ£€æŸ¥Kçº¿æ•°æ®è¶…æ—¶ï¼ˆä»ç„¶ä½¿ç”¨Redisï¼Œå› ä¸ºæœ‰å¤šä¸ªtimeframeï¼‰
                    for timeframe in self.timeframes:
                        last_update = self.data_manager.get_kline_last_update(symbol, timeframe)
                        if last_update:
                            time_since_update_ms = now_ms - last_update
                            if time_since_update_ms > timeout_threshold_ms:
                                timeout_detected = True
                                timeout_details.append(
                                    f"Kçº¿{timeframe}: {time_since_update_ms/1000:.0f}ç§’"
                                )

                    # 2. æ£€æŸ¥é€ç¬”æˆäº¤æ•°æ®è¶…æ—¶ï¼ˆä»æ•°æ®åº“è·å–æœ€æ–°timestampï¼‰
                    last_trade_ts = self.data_manager.get_latest_trade_timestamp_from_redis(symbol)
                    if last_trade_ts:
                        time_since_trades_ms = now_ms - last_trade_ts
                        if time_since_trades_ms > timeout_threshold_ms:
                            timeout_detected = True
                            timeout_details.append(
                                f"é€ç¬”æˆäº¤: {time_since_trades_ms/1000:.0f}ç§’"
                            )

                    # 3. æ£€æŸ¥è®¢å•ç°¿æ•°æ®è¶…æ—¶ï¼ˆä»Redisè·å–æœ€æ–°timestampï¼‰
                    orderbook_redis = self.data_manager.get_orderbook_from_redis(symbol, depth=1)
                    if orderbook_redis and orderbook_redis.get('timestamp'):
                        time_since_orderbook_ms = now_ms - orderbook_redis['timestamp']
                        if time_since_orderbook_ms > timeout_threshold_ms:
                            timeout_detected = True
                            timeout_details.append(
                                f"è®¢å•ç°¿: {time_since_orderbook_ms/1000:.0f}ç§’"
                            )



                # å¦‚æœæ£€æµ‹åˆ°ä»»ä½•æ•°æ®æºè¶…æ—¶ï¼Œè§¦å‘é‡å¯
                if timeout_detected:
                    logger.error(
                        f"âŒ æ•°æ®æ›´æ–°è¶…æ—¶æ£€æµ‹åˆ°ï¼ä»¥ä¸‹æ•°æ®æºè¶…è¿‡ {self.data_timeout_seconds}ç§’ æœªæ›´æ–°ï¼š"
                    )
                    for detail in timeout_details:
                        logger.error(f"   - {detail}")
                    logger.warning("ğŸ”„ WebSocketå¯èƒ½å·²æ–­å¼€æˆ–æ•°æ®æµä¸­æ–­ï¼Œå‡†å¤‡é‡å¯é‡‡é›†å™¨...")

                    # è®¾ç½®é‡å¯æ ‡å¿—å¹¶åœæ­¢å½“å‰è¿è¡Œ
                    self.need_restart = True
                    self.stop()
                    logger.warning("âš ï¸ é‡‡é›†å™¨å·²åœæ­¢ï¼Œå°†åœ¨5ç§’åè‡ªåŠ¨é‡å¯...")
                    return  # é€€å‡ºç›‘æ§å¾ªç¯

                # æ£€æŸ¥æ•´ä½“æ•°æ®æ›´æ–°ï¼ˆå‘åå…¼å®¹ï¼Œä½¿ç”¨stats['last_update']ï¼‰
                if self.stats['last_update']:
                    time_since_update = (datetime.now() - self.stats['last_update']).total_seconds()

                    if time_since_update > self.data_timeout_seconds * 0.7:
                        # æå‰è­¦å‘Šï¼ˆè¶…è¿‡70%é˜ˆå€¼æ—¶ï¼‰
                        logger.warning(
                            f"âš ï¸ æ•°æ®æ›´æ–°å»¶è¿Ÿè­¦å‘Š: å·²ç» {time_since_update:.0f} ç§’æœªæ”¶åˆ°æ–°æ•°æ® "
                            f"(é˜ˆå€¼: {self.data_timeout_seconds}ç§’)"
                        )

                # æ‰“å°çŠ¶æ€
                logger.info("\n" + "=" * 60)
                logger.info(f"ğŸ“Š æ•°æ®é‡‡é›†çŠ¶æ€ [{datetime.now().strftime('%H:%M:%S')}]")
                logger.info("=" * 60)

                # ç»Ÿè®¡æ¯ä¸ªäº¤æ˜“å¯¹çš„æ•°æ®é‡
                for symbol in self.symbols:
                    logger.info(f"\näº¤æ˜“å¯¹: {symbol}")

                    # Kçº¿æ•°æ®åŠæœ€åæ›´æ–°æ—¶é—´
                    for tf in self.timeframes:
                        klines = self.data_manager.get_recent_klines(symbol, tf, limit=1)
                        last_update = self.data_manager.get_kline_last_update(symbol, tf)
                        if klines:
                            latest = klines[0]
                            update_status = ""
                            if last_update:
                                seconds_ago = (now_ms - last_update) / 1000
                                update_status = f", æ›´æ–°äº{seconds_ago:.0f}ç§’å‰"
                            logger.info(
                                f"  {tf} Kçº¿: æœ€æ–°ä»·æ ¼ {latest['close']:.2f}, "
                                f"æ—¶é—´ {datetime.fromtimestamp(latest['timestamp']/1000).strftime('%H:%M:%S')}"
                                f"{update_status}"
                            )

                    # é€ç¬”æˆäº¤åŠæœ€åæ›´æ–°æ—¶é—´ï¼ˆä»Redisè·å–ï¼‰
                    last_trade_ts = self.data_manager.get_latest_trade_timestamp_from_redis(symbol)
                    if last_trade_ts:
                        seconds_ago = (now_ms - last_trade_ts) / 1000
                        logger.info(f"  é€ç¬”æˆäº¤: æ›´æ–°äº{seconds_ago:.0f}ç§’å‰ (Redisç¼“å­˜)")

                    # è®¢å•ç°¿åŠæœ€åæ›´æ–°æ—¶é—´ï¼ˆä»Redisè·å–ï¼‰
                    orderbook_redis = self.data_manager.get_orderbook_from_redis(symbol, depth=1)
                    if orderbook_redis and orderbook_redis.get('timestamp'):
                        # è®¡ç®—æ›´æ–°æ—¶é—´
                        seconds_ago = (now_ms - orderbook_redis['timestamp']) / 1000
                        update_status = f", æ›´æ–°äº{seconds_ago:.0f}ç§’å‰"

                        # è·å–ä»·æ ¼ä¿¡æ¯
                        bids = orderbook_redis.get('bids', [])
                        asks = orderbook_redis.get('asks', [])

                        if bids and asks:
                            bid1_price = float(bids[0][0])
                            ask1_price = float(asks[0][0])
                            spread_pct = (ask1_price - bid1_price) / bid1_price * 100

                            logger.info(
                                f"  è®¢å•ç°¿: ä»·å·®={spread_pct:.8f}%, "
                                f"ä¹°1={bid1_price:.2f}, "
                                f"å–1={ask1_price:.2f}"
                                f"{update_status}"
                            )

                logger.info("\nâœ“ æ•°æ®é‡‡é›†æ­£å¸¸è¿è¡Œä¸­...")

                # æ˜¾ç¤ºæœ€åæ›´æ–°æ—¶é—´
                if self.stats['last_update']:
                    logger.info(
                        f"ğŸ“¡ æœ€åæ•°æ®æ›´æ–°: {self.stats['last_update'].strftime('%H:%M:%S')} "
                        f"({(datetime.now() - self.stats['last_update']).total_seconds():.0f}ç§’å‰)"
                    )

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"çŠ¶æ€ç›‘æ§å¼‚å¸¸: {e}")

    def stop(self):
        """åœæ­¢æ•°æ®é‡‡é›†"""
        if not self.is_running:
            return

        logger.info("\n" + "=" * 60)
        logger.info("ğŸ›‘ åœæ­¢æ•°æ®é‡‡é›†å™¨")
        logger.info("=" * 60)

        self.is_running = False

        # å–æ¶ˆæ‰€æœ‰åå°ä»»åŠ¡ï¼ˆç«‹å³åœæ­¢æ‰€æœ‰å¾ªç¯ï¼Œä¸ç­‰å¾…sleepç»“æŸï¼‰
        if hasattr(self, 'background_tasks') and self.background_tasks:
            logger.info("æ­£åœ¨å–æ¶ˆæ‰€æœ‰åå°ä»»åŠ¡...")
            for task in self.background_tasks:
                if not task.done():
                    task.cancel()
            logger.info(f"âœ“ å·²å–æ¶ˆ {len(self.background_tasks)} ä¸ªåå°ä»»åŠ¡")

        # å…³é—­ WebSocket è¿æ¥
        if self.ws_public:
            self.ws_public.close()
        if self.ws_business:
            self.ws_business.close()

        # å…³é—­çº¿ç¨‹æ± 
        logger.info("æ­£åœ¨å…³é—­çº¿ç¨‹æ± ...")
        self.executor.shutdown(wait=True, cancel_futures=True)

        logger.success("âœ“ æ•°æ®é‡‡é›†å™¨å·²åœæ­¢")
        logger.info(f"åœæ­¢æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    def get_stats(self) -> dict:
        """è·å–é‡‡é›†ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'is_running': self.is_running,
            'symbols': self.symbols,
            'timeframes': self.timeframes,
            'stats': self.stats,
            'data': {}
        }

        for symbol in self.symbols:
            symbol_stats = {
                'klines': {},
                'trades': {},
                'orderbook': {}
            }

            # Kçº¿ç»Ÿè®¡
            for tf in self.timeframes:
                klines = self.data_manager.get_recent_klines(symbol, tf, limit=100)
                symbol_stats['klines'][tf] = len(klines) if klines else 0

            # æˆäº¤ç»Ÿè®¡
            for interval, key in [(60, '1m'), (300, '5m'), (900, '15m')]:
                pressure = self.data_manager.get_latest_pressure(symbol, interval)
                symbol_stats['trades'][key] = pressure is not None

            # è®¢å•ç°¿ç»Ÿè®¡
            orderbook = self.data_manager.get_latest_orderbook(symbol)
            symbol_stats['orderbook']['available'] = orderbook is not None

            stats['data'][symbol] = symbol_stats

        return stats


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='OKXç‹¬ç«‹å®æ—¶æ•°æ®é‡‡é›†å™¨ - æŒç»­é‡‡é›†Kçº¿ã€é€ç¬”æˆäº¤ã€è®¢å•ç°¿æ•°æ®'
    )

    parser.add_argument(
        '--symbols',
        type=str,
        default='BTC-USDT-SWAP',
        help='äº¤æ˜“å¯¹åˆ—è¡¨ï¼Œé€—å·åˆ†éš”ï¼ˆé»˜è®¤: BTC-USDT-SWAPï¼‰'
    )

    parser.add_argument(
        '--timeframes',
        type=str,
        default='1m,5m,15m',
        help='Kçº¿å‘¨æœŸåˆ—è¡¨ï¼Œé€—å·åˆ†éš”ï¼ˆé»˜è®¤: 1m,5m,15mï¼‰'
    )

    parser.add_argument(
        '--history-days',
        type=int,
        default=30,
        help='åˆå§‹åŒ–å†å²æ•°æ®å¤©æ•°ï¼ˆé»˜è®¤: 30ï¼‰'
    )

    parser.add_argument(
        '--data-timeout',
        type=int,
        default=120,
        help='æ•°æ®æ›´æ–°è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œè¶…è¿‡æ­¤æ—¶é—´æœªæ”¶åˆ°æ•°æ®åˆ™é‡å¯ï¼ˆé»˜è®¤: 120ï¼‰'
    )

    parser.add_argument(
        '--max-restarts',
        type=int,
        default=9999,
        help='æœ€å¤§è‡ªåŠ¨é‡å¯æ¬¡æ•°ï¼ˆé»˜è®¤: 10ï¼‰ï¼Œè¶…è¿‡æ­¤æ¬¡æ•°åç¨‹åºé€€å‡º'
    )

    parser.add_argument(
        '--status-interval',
        type=int,
        default=60,
        help='çŠ¶æ€æ‰“å°é—´éš”ï¼ˆç§’ï¼Œé»˜è®¤: 60ï¼‰'
    )

    args = parser.parse_args()

    # è§£æå‚æ•°
    symbols = [s.strip() for s in args.symbols.split(',')]
    timeframes = [tf.strip() for tf in args.timeframes.split(',')]

    # é‡å¯è®¡æ•°å™¨
    restart_count = 0
    max_restarts = args.max_restarts

    # è‡ªåŠ¨é‡å¯å¾ªç¯
    while restart_count <= max_restarts:
        if restart_count > 0:
            logger.warning(f"ğŸ”„ ç¬¬ {restart_count} æ¬¡é‡å¯ (æœ€å¤š {max_restarts} æ¬¡)")
            await asyncio.sleep(5)  # ç­‰å¾…5ç§’åé‡å¯

        # åˆ›å»ºå¹¶å¯åŠ¨é‡‡é›†å™¨
        collector = StandaloneDataCollector(
            symbols=symbols,
            timeframes=timeframes,
            history_days=args.history_days,
            data_timeout_seconds=args.data_timeout
        )

        try:
            await collector.start()

            # æ­£å¸¸ç»“æŸï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦é‡å¯
            if collector.need_restart:
                restart_count += 1
                logger.info(f"âœ“ å‡†å¤‡é‡å¯é‡‡é›†å™¨... (å·²é‡å¯ {restart_count}/{max_restarts} æ¬¡)")
                continue
            else:
                # ç”¨æˆ·æ­£å¸¸é€€å‡ºï¼Œä¸é‡å¯
                logger.info("âœ“ é‡‡é›†å™¨æ­£å¸¸é€€å‡º")
                break

        except KeyboardInterrupt:
            logger.info("\nâš ï¸ ç”¨æˆ·ä¸­æ–­ï¼Œåœæ­¢é‡‡é›†å™¨...")
            collector.stop()
            break

        except Exception as e:
            logger.error(f"âŒ ç¨‹åºå¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()

            # å¼‚å¸¸é€€å‡ºä¹Ÿå°è¯•é‡å¯
            restart_count += 1
            if restart_count <= max_restarts:
                logger.warning(f"ğŸ”„ å› å¼‚å¸¸å‡†å¤‡é‡å¯... (å·²é‡å¯ {restart_count}/{max_restarts} æ¬¡)")
                collector.stop()
                continue
            else:
                logger.error(f"âŒ å·²è¾¾åˆ°æœ€å¤§é‡å¯æ¬¡æ•° ({max_restarts})ï¼Œç¨‹åºé€€å‡º")
                collector.stop()
                break

        finally:
            # ç¡®ä¿èµ„æºæ¸…ç†
            if collector.is_running:
                collector.stop()

    if restart_count > max_restarts:
        logger.critical(f"âš ï¸ é‡‡é›†å™¨é‡å¯æ¬¡æ•°è¶…è¿‡é™åˆ¶ ({max_restarts})ï¼Œç¨‹åºç»ˆæ­¢")
        sys.exit(1)


if __name__ == '__main__':

    # é…ç½®æ—¥å¿—æ ¼å¼
    logger.remove()
    logger.add(
        sys.stderr,
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>",
        level="INFO"
    )

    # æ·»åŠ æ–‡ä»¶æ—¥å¿—
    logger.add(
        "logs/data_collector_{time:YYYY-MM-DD}.log",
        rotation="00:00",
        retention="7 days",
        level="INFO"
    )

    logger.info("=" * 60)
    logger.info("OKX ç‹¬ç«‹å®æ—¶æ•°æ®é‡‡é›†å™¨ v2.0")
    logger.info("ä½¿ç”¨ OkxWebSocket ç±» + ThreadPoolExecutor")
    logger.info("=" * 60)
    logger.info(f"collector PID: {os.getpid()}")
    COLLECTOR_PID_FILE = os.path.join(project_root, "data", "collector.pid")

    with open(COLLECTOR_PID_FILE, 'w') as f:
        f.write(str(os.getpid()))

    # è¿è¡Œ
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("\nç¨‹åºé€€å‡º")
    except Exception as e:
        logger.error(f"å¯åŠ¨å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
